{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd  \n\nfrom sklearn.metrics import confusion_matrix\n\nimport os\nfrom torch.utils.data import DataLoader, Dataset\n\nimport torch,torchaudio\n\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n#from torchmetrics import Accuracy\nfrom einops import rearrange\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom torchinfo import summary\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup\nfrom speechbrain.lobes.models.ECAPA_TDNN import AttentiveStatisticsPooling","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os ,random\nimport numpy as np\nos.environ['PYTHONHASHSEED'] = '42'\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    def __init__(self,dataset_name):\n        self.dataset_name = dataset_name\n        self._config = self._select_config()\n    def _select_config(self):\n        if self.dataset_name == 'Emodb':\n            return {\n             'batch_size':16,\n            'dataset':self.dataset_name,\n            'session': 'emodb_session',\n            'class_label_name' : 'emodb_emotion',\n            'dataset_path' : 'niloufarr/Dataset/emodb-dataset:v0',\n            'csv_path' : 'niloufarr/Dataset/pairs-emodb_imocap:v1',\n            \"sessionList\": [ 3, 8,  9, 10, 11, 12, 13, 14, 15, 16],\n                 \"x_name\":'emodb',\n            \"loss\" :768\n\n            }\n    def get_dataset(self):\n        return self.dataset_name.split(\"_\")\n    def get_property(self, property_name):\n        if not self._config or property_name not in self._config:\n            print(f\"{property_name} not found in {self.dataset_name} configuration!!!\")\n            return None\n        return self._config[property_name]\n\n\ninstance_config = Config(\"Emodb\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key='')\nsweep_config = {\n    'method':'grid'\n}\nparameters_dict = {\n    'fold':{\n        'values': [[3,8,9,10],[10,11,12,13],[13,14,15,16]]\n    }\n}\nparameters_dict.update({\n     'project_name':{\n        'value':\"Finetune DistilHuBERT\"\n    },\n    'epochs':{\n      'value': 35\n    },\n    'batch_size':{\n        'value':  instance_config.get_property('batch_size')\n    },\n    'learning_rate':{\n        'value': 0.0003#5e-3\n    },\n    'weight_decay':{\n        'value': 9e-3\n    },\n    'gpu':{\n        'value':'P100'\n    },\n    'loss':{\n        'value':'focal'\n    },\n    'optimizer':{\n        'value': 'AdamW'\n    },\n    'class_number':{\n        'value': 4\n    },\n    'class_names':{\n        'value':['neutral', 'happy', 'angry', 'sad']\n    },\n    'valid_labels':{\n        'value':['0', '1', '2', '3']\n    },\n    'dataset':{\n        'value':instance_config.get_property(\"dataset\")\n    },\n    'dataset_path':{\n        'value':instance_config.get_property(\"dataset_path\")\n    },\n    'csv_path':{\n        'value':instance_config.get_property(\"csv_path\")\n    },\n    \n    'model_path':{\n        'value':instance_config.get_property(\"modelPath\") if instance_config.get_property(\"modelPath\")!=None else '-'\n    },\n    'asp_att_dim':{\n        'value':768\n    },\n    'fc':{\n        'value':256\n    },\n    'maxout_num_linear_function':{\n        'value':3\n    },\n    'freeze':{\n        'value':True\n    },\n    'freezeList':{\n        'value':[\n#             'model.model.model.feature_extractor.conv_layers',\n                  'model.model.model.encoder.layers.0',\n                 'model.model.model.encoder.layers.1',\n    'model.model.model.post_extract_proj',\n                'model.model.model.encoder.pos_conv.0',\n                ]\n    },\n\n    'description':{\n        'value':'whole distilhebert asp maxout layerNorm dropout'\n    },\n    'description2':{\n        'value':f\"finetune with {len(parameters_dict['fold']['values'][0])} speakers  update cnn freeze transformers\"\n    }\n    })\nsweep_config['parameters'] = parameters_dict\n","metadata":{"_uuid":"acf5eb79-6b2b-4332-9203-5a56446aa3e5","_cell_guid":"6929cc76-1916-437e-91f2-cdf54da17324","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_id = input('What is sweep_id? (leave out if this is first sweep) ')\nif sweep_id==\"\":\n    sweep_id = wandb.sweep(sweep_config, project=parameters_dict['project_name']['value'])\nsaveModel = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_waveform(file_path, sample_rate=16000):\n      wf, sr = torchaudio.load(file_path)\n      resample = torchaudio.transforms.Resample(sr, sample_rate)\n      waveform = resample(wf)\n      if waveform.shape[0]>1:\n        waveform = waveform.mean(dim=0,keepdim=True)\n      return waveform\ndef tile(waveform, expected_time):\n  waveform_time = waveform.shape[1]\n  expected_time = expected_time\n  repeat_times = (expected_time // waveform_time) + 1\n  tiled_data = waveform.repeat(1, repeat_times)  \n  return tiled_data[:, :expected_time]\n\nclass CaseInSensitiveDict(dict):\n    def __getitem__(self,key):\n        for k in self.keys():\n            if key.lower()== k.lower():\n                return super().__getitem__(k)\n        raise  KeyError(key)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_labels(labels):\n    emotion_label_dict={\n        'neutral':0,\n        'happy':1,\n        'angry':2,\n        'sad':3,\n        'anger':2,\n    }\n    \n    case_insensitive_dict = CaseInSensitiveDict(emotion_label_dict)\n    return [case_insensitive_dict[emo] for emo in labels]\n\ndef decode_labels(y):\n    emotion_mapping={\n        0:'neutral',\n        1:'happy',\n        2:'angry',\n        3:'sad'\n       \n    }\n    \n    \n    return [emotion_mapping[emo] for emo in y]","metadata":{"_uuid":"f289cdb0-7cf3-4d65-b7ba-36bcd78183e5","_cell_guid":"58897c78-fa46-4beb-904a-21379c8e252e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-07T10:33:56.591145Z","iopub.execute_input":"2024-04-07T10:33:56.591748Z","iopub.status.idle":"2024-04-07T10:33:56.600202Z","shell.execute_reply.started":"2024-04-07T10:33:56.591717Z","shell.execute_reply":"2024-04-07T10:33:56.599399Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClassifierDataset(Dataset):\n    def __init__(self,csv_file_name,path_dataset,sessionList,x_name,sessionListName,class_label_name):\n        \n        df=pd.read_csv(csv_file_name)\n        self.path_dataset=path_dataset\n        print(\"sessionList\",sessionList)\n        self.temp_df=df[df[sessionListName].isin( sessionList)].reset_index(drop=True)\n        print(\"self.temp_df size\",self.temp_df.shape)\n        self.speech_path=self.temp_df[x_name].values\n        self.label=encode_labels(self.temp_df[class_label_name].values)\n        \n        \n    def __getitem__(self,index):\n        \n        speech=tile(get_waveform(os.path.join(self.path_dataset,self.speech_path[index])),\n                     16000*7)\n        return speech, torch.tensor(self.label[index],dtype=torch.long)\n    \n    def __len__(self):\n        print(\"self.temp_df.shape[0]\",self.temp_df.shape[0])\n        return self.temp_df.shape[0]","metadata":{"_uuid":"1f664480-2e44-489a-9180-6fa1be67fe9c","_cell_guid":"529e1537-5e37-4e72-9caa-7b2aa174517d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-07T10:33:56.610957Z","iopub.execute_input":"2024-04-07T10:33:56.611214Z","iopub.status.idle":"2024-04-07T10:33:56.620913Z","shell.execute_reply.started":"2024-04-07T10:33:56.611192Z","shell.execute_reply":"2024-04-07T10:33:56.620041Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ASP(nn.Module):\n\n    def __init__(self, num_emdb, attn_dim=None,split=True):\n        super().__init__()\n        self.channels = num_emdb\n        self.split = split\n        if not attn_dim:\n          attn_dim = num_emdb\n        self.asp = AttentiveStatisticsPooling(channels= self.channels, attention_channels=attn_dim, global_context=True)\n\n    def forward(self, x: torch.Tensor):\n\n        x = rearrange(x, \"b l c -> b c l\")\n        x = self.asp(x)\n#         x = x.squeeze()\n        x=torch.squeeze(x,2)\n        if self.split :\n            x, _ = torch.split(x, self.channels, dim=1)\n                \n        return x\n\nclass Maxout(nn.Module):\n    def __init__(self, in_features, out_features, num_linear_function=2):\n        super(Maxout, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_linear_function = num_linear_function\n        self.fc = nn.Linear(in_features, out_features * num_linear_function)\n\n    def forward(self, x):\n        x = self.fc(x)\n        x, _ = torch.max(x.view(-1, self.out_features, self.num_linear_function), dim=2)\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self,model,config):\n        super(Classifier,self).__init__()\n        self.model= model\n        \n        self.asp=ASP(num_emdb = instance_config.get_property('loss'), attn_dim=config.asp_att_dim)\n        self.maxout = Maxout(in_features= instance_config.get_property('loss'), out_features= config.fc,num_linear_function=config.maxout_num_linear_function)\n        self.maxout_norm = nn.LayerNorm(normalized_shape = config.fc)\n        self.dropout = nn.Dropout(config.proj_drop)\n        self.fc3=nn.Linear(config.fc,config.class_number)#768,4\n                \n    def forward(self,x):\n        x = self.model(x)[\"paper\"]\n        x = self.asp(x)\n        x = self.maxout(x)\n        x = self.maxout_norm(x)\n        x = self.dropout(x)\n        e = x.clone()\n        x=self.fc3(x)\n        return x,e","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha = 0.8, gamma = 2, class_number =4):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.class_number = class_number\n    def forward(self, inputs, targets,*args):\n        targets = torch.nn.functional.one_hot(targets ,num_classes=self.class_number)\n        #comment out if your model contains a sigmoid or equivalent activation layer\n        sig = nn.Sigmoid()\n        inputs = sig(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        inputs=inputs.to(torch.float32)\n        targets = targets.view(-1)\n        targets=targets.to(torch.float32)\n        \n        #first compute binary cross-entropy \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = self.alpha * (1-BCE_EXP)**self.gamma * BCE\n                       \n        return focal_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_optimizer(net,config):\n    \n    if config.optimizer=='AdamW':\n        optimizer =torch.optim.AdamW(net.parameters(),\n                                  lr = config.learning_rate,\n                                  weight_decay = config.weight_decay)\n    if config.optimizer == 'SGD':\n        optimizer =torch.optim.SGD(lr=config.learning_rate\n                                  ,weight_decay = config.weight_decay)\n    return optimizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_classifier(config):\n   net = initialization_weights(config.initialize_weight)\n   classifierModel = Classifier(net,config).to(device)\n        \n   if config.freeze:\n       freezeTuple = tuple(config.freezeList)\n       for name,params in classifierModel.named_parameters(): \n            if name.startswith(freezeTuple):\n                params.requires_grad = False\n            print(name,params.requires_grad)\n   return classifierModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make(config):\n    if torch.cuda.device_count()>1:\n        net = nn.DataParallel(get_classifier(config),device_ids=[0, 1])\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    else:\n        net = get_classifier(config)\n    net.to(device)\n    criterion = get_loss(config)\n    optimizer = get_optimizer(net,config)\n    return net,criterion,optimizer\n\n    file_name = list(filter(lambda f:f.endswith(\".csv\"),os.listdir(artifact_dir_csv)))[0]\n    classifier_dataset = ClassifierDataset(csv_file_name=os.path.join(artifact_dir_csv,file_name),\n                                       path_dataset=artifact_dir,\n                                       sessionList=config.fold,\n                                       x_name=instance_config.get_property(\"x_name\"),\n                                       sessionListName= instance_config.get_property(\"session\"),\n                                       class_label_name=instance_config.get_property(\"class_label_name\"))\n    train_dataloader = DataLoader(classifier_dataset, batch_size=config.batch_size, num_workers=2, shuffle=True)\n    return train_dataloader\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_test_dataloader(dataset_path,csv_directory,sessionName,sessionList,batch_size,x_name='emodb'):\n#     directory_path = artifact_dir_source_csv if x_name in artifact_dir_source_csv else artifact_dir_target_csv\n    file_name = list(filter(lambda f:f.endswith(\".csv\"),os.listdir(csv_directory)))[0]\n    classifier_dataset = ClassifierDataset(csv_file_name=os.path.join(csv_directory, file_name),\n                                        path_dataset=dataset_path,#config.target_dataset_path,\n                                       sessionList= sessionList,\n                                       x_name=x_name,#'emodb',\n                                       sessionListName=sessionName,#x_name+\"_session\",\n                                       class_label_name=instance_config.get_property('class_label_name'))#x_name+'_emotion')#'emodb_emotion')\n\n\n    test_dataloader = DataLoader(classifier_dataset, num_workers=2, batch_size=batch_size, shuffle=False)\n#     print(\"test_dataloader len\",len(classifier_dataset))\n    return test_dataloader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_sessionList(property_name:str,fold:list):\n    '''Flatten session list.\n        Args:\n            property_name (str): session list name (source or target).\n            fold (list): session should be removed from session list.\n        Returns\n            list : flattened sessionList\n    '''\n    sessionSet = set(instance_config.get_property(property_name))\n    sessionList = list(sessionSet.difference(set(fold)))\n    return sessionList","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score,accuracy_score\nimport copy\ndef test(config,datasetName,test_dataloader,*net):\n    y_pred = []\n    y_true = []\n    flag=True\n    if len(net)==0:\n       net = load_state_dict(config) \n    else:\n        net=net[0]\n        flag=False\n    \n    for i, (speech, label) in enumerate(test_dataloader):\n      speech = speech.squeeze(1)\n      inputs, labels = speech.to(device), label.to(device)\n      outputs,embeddings = net(inputs)\n      _, predicted = torch.max(outputs.data, 1)\n      y_true += labels.tolist()\n      y_pred += predicted.tolist()\n          \n    WA=balanced_accuracy_score(y_true, y_pred)\n    UA=accuracy_score(y_true, y_pred)\n    wandb.log({\n          \"{} Weighted Accuracy\".format(datasetName): WA,\n          \"{} Unweighted Accuracy\".format(datasetName): UA\n          })\n    \n    if flag:\n        label = datasetName if datasetName.isupper() else datasetName[0].upper() + datasetName[1:]\n        log_confusion_matrix(y_pred,y_true,label)\n\n       \n    return WA,UA\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model(config):\n        pickle_artifact = wandb.Artifact(\n            name = f'model.pkl',\n            type=\"model\",\n            metadata=dict(config))\n        wandb.save(PATH)\n\n        pickle_artifact.add_file(PATH)\n\n\n        wandb.log_artifact(pickle_artifact)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATH='model.pkl'\ndef train(config,net,criterion,optimizer):\n    best_accuracy=0\n    best_VUA = 0\n    accuracy=0\n    accuracyArr = np.array([])\n    train_dataloader= make_train_loader(config)\n    wandb.watch(net, criterion, log=\"all\", log_freq=1,log_graph=False)\n    net.train()\n    print(\"validation\")\n    test_dataloader1 =make_test_dataloader(artifact_dir,csv_directory=artifact_dir_csv\n                                           ,sessionName=instance_config.get_property(\"session\")\n                                            ,sessionList= get_sessionList(\"sessionList\",config.fold)\n                                           ,batch_size = config.batch_size\n                                           ,x_name=instance_config.get_property(\"x_name\"))\n    # Iterate throught the epochs\n    for epoch in range(config.epochs):\n        train_loss = []\n        train_acc  = []\n        # Iterate over batches\n        for i, (speech,label) in enumerate(train_dataloader):\n            \n            # Send the speech and labels to CUDA\n            print(\"speech\", speech.shape)\n            speech, label = speech.to(device), label.to(device)\n            speech = speech.squeeze(1)\n            # Zero the gradients (PyTorch accumulates the gradients on subsequent backward passes. \n            #This accumulating behaviour is convenient while training RNNs or when we want to compute the gradient of the loss summed over multiple mini-batches.)\n            optimizer.zero_grad()\n\n\n\n\n            # Pass in the two speeches into the network and obtain two outputs and classifier output for the  source speech\n            output,embedings = net(speech)\n            _,predicted=torch.max(output.data,1)\n            loss = get_loss_param(criterion,output,embedings,label)\n\n            # Calculate the backpropagation\n            loss.backward()\n            # Optimize\n            optimizer.step()\n            correct_predictions = torch.sum(predicted == label)\n            number_of_predictions = torch.numel(predicted) #int. Returns the total number of elements in the input tensor.\n            accuracy = correct_predictions/number_of_predictions\n            train_loss.append(loss.item())\n            train_acc.append(accuracy.item())\n            print(\"loss \" ,loss.item())\n            print(\"accuracy \",accuracy.item())\n            print(\"******************\")\n        trainAccuracy=np.mean(train_acc)\n        print(\"epoch {} accuracy {}\".format(epoch,trainAccuracy))\n        wandb.log({\n            \"Train loss\": np.mean(train_loss), \n            \"Train Accuracy\": trainAccuracy,\n            \"epoch\":epoch+1\n            })\n        print(\"&&&&&&&&&&&&&&&&&&&&&&&&&\\n validation\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n        \n        _,VUA = test(config,\"Validation \"+instance_config.get_property(\"x_name\"),test_dataloader1,net)\n        \n        print(f\"epoch {epoch} vua\",VUA)\n        print(\"&&&&&&&&&&&&&&&&&&&&&&&&&\\n validation\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n        \n        if trainAccuracy > best_accuracy:\n            best_accuracy =trainAccuracy\n            if trainAccuracy != 1:\n                best_VUA = VUA\n            print(\"epoch {} best accuracy {}\".format(epoch,best_accuracy))\n            if torch.cuda.device_count()>1:\n                torch.save(net.module.state_dict(),PATH)\n\n            else:  \n                torch.save(net.state_dict(), PATH)\n        if trainAccuracy == 1 and VUA > best_VUA:\n            print(\"best vua\",VUA)\n            best_VUA = VUA\n            if torch.cuda.device_count()>1:\n                torch.save(net.module.state_dict(),PATH)\n\n            else:  \n                torch.save(net.state_dict(), PATH)\n            \n        if(epoch > 16):\n            check , accuracyArr = isEarlyStopping(accuracyArr,trainAccuracy)\n            \n            if(check):\n                print(\"break in epoch:\", epoch)\n                break\n        else: \n            accuracyArr = np.append(accuracyArr,trainAccuracy)\n        print(\"---------------------------\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_model_pipeline(hyperparameters=None):\n    with wandb.init(config=hyperparameters):\n        # access all HPs through wandb.config, so logging matches execution!\n        config = wandb.config\n        \n        net,criterion,optimizer=make(config) \n        # train model\n        train(config,net,criterion,optimizer)\n        if saveModel:\n            save_model(config)\n        print(\"____________________________________\")\n        print(\"***************************\")\n        #Test on target dataset\n        test_dataloader =make_test_dataloader(artifact_dir,csv_directory=artifact_dir_csv,\n                                              sessionName=instance_config.get_property(\"session\")\n                                               ,sessionList = get_sessionList(\"sessionList\",config.fold)\n                                              ,batch_size = config.batch_size\n                                              ,x_name=instance_config.get_property(\"x_name\"))\n      \n        try:\n            print(summary(net,input_size=(config.batch_size, config.input_size[0], config.input_size[1])))\n        except:\n            print(\"something is wrong with torchinfo\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.agent(sweep_id,project=parameters_dict['project_name']['value'],entity='',function=make_model_pipeline)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}